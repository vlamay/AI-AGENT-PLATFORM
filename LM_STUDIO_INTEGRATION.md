# ğŸ¤– LM Studio Integration Guide

## ğŸ“‹ Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ LM Studio?

**LM Studio** - ÑÑ‚Ğ¾ Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… LLM Ñ Ğ²ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¼ API ÑĞµÑ€Ğ²ĞµÑ€Ğ¾Ğ¼, Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ° Ollama.

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° LM Studio:**
- âœ… Ğ“Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑĞ¼Ğ¸
- âœ… ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° GGUF/GGML Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ²
- âœ… ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ UI Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
- âœ… Ğ’ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ½Ñ‹Ğ¹ OpenAI-ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ñ‹Ğ¹ API
- âœ… ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° CUDA Ğ´Ğ»Ñ NVIDIA GPU

---

## ğŸš€ Ğ¨Ğ°Ğ³ 1: Ğ—Ğ°Ğ¿ÑƒÑĞº LM Studio API

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ A: Ğ§ĞµÑ€ĞµĞ· Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ

1. **ĞÑ‚ĞºÑ€Ğ¾Ğ¹Ñ‚Ğµ LM Studio** (ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½)

2. **Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚Ğµ Local Server**:
   - ĞĞ°Ğ¹Ğ´Ğ¸Ñ‚Ğµ Ñ€Ğ°Ğ·Ğ´ĞµĞ» "Local Server" Ğ² Ğ±Ğ¾ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ°Ğ½ĞµĞ»Ğ¸
   - ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ "Start Server"
   - Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ñ€Ñ‚ (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 1234)
   - Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ ÑĞµÑ€Ğ²ĞµÑ€

3. **Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ**:
   - Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ½ÑƒĞ¶Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ· Ğ±Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ¸
   - ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ½ĞµÑ‘ Ğ² Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹ÑĞµ

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ B: Ğ§ĞµÑ€ĞµĞ· ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒ

```bash
# Ğ•ÑĞ»Ğ¸ LM Studio ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ĞºĞ°Ğº AppImage
/opt/LM-Studio.AppImage --server 1234

# Ğ˜Ğ»Ğ¸ ĞµÑĞ»Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ñ‡ĞµÑ€ĞµĞ· apt/flatpak
lm-studio --server 1234
```

---

## ğŸ§ª Ğ¨Ğ°Ğ³ 2: Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ LM Studio API

### ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸

```bash
# Ğ¢ĞµÑÑ‚ API
curl http://localhost:1234/v1/models

# Ğ”Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ²ĞµÑ€Ğ½ÑƒÑ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
```

### Ğ¢ĞµÑÑ‚ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-model",
    "messages": [{"role": "user", "content": "Hello!"}],
    "temperature": 0.7
  }'
```

---

## ğŸ”§ Ğ¨Ğ°Ğ³ 3: Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ A: Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ollama Ğ½Ğ° LM Studio

Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ `docker-compose-lm-studio.yml`:

```yaml
version: '3.8'

services:
  # ============================================
  # LM Studio Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ollama
  # ============================================
  
  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ‡Ğ°Ğ½Ğ¸Ğµ: LM Studio Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒÑÑ
  # Ğ½Ğ° Ñ…Ğ¾ÑÑ‚Ğµ (Ğ½Ğµ Ğ² Docker), Ñ‚Ğ°Ğº ĞºĞ°Ğº ÑÑ‚Ğ¾ GUI Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ
  
  # ============================================
  # FastAPI Orchestrator Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ LM Studio
  # ============================================
  api:
    build:
      context: ./services/orchestrator
      dockerfile: Dockerfile
    container_name: ai-platform-api-lm
    ports:
      - "8080:8080"
    environment:
      - LM_STUDIO_ENDPOINT=http://host.docker.internal:1234
      - OLLAMA_ENDPOINT=http://ollama:11434
      - PREFERRED_LLM=lm_studio  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LM Studio ĞºĞ°Ğº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./services/orchestrator/app:/app
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  # ĞÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ¾ÑÑ‚Ğ°ÑÑ‚ÑÑ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
  postgres:
    image: postgres:16-alpine
    container_name: ai-platform-db
    environment:
      POSTGRES_DB: ai_platform
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres_secure_pass_2024
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: ai-platform-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  n8n:
    image: n8nio/n8n:latest
    container_name: ai-platform-n8n
    ports:
      - "5679:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=ai_platform_2024
    volumes:
      - n8n_data:/home/node/.n8n
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  n8n_data:
```

---

### Ğ’Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ B: Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (LM Studio + Ollama)

ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ `main_enhanced.py` Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ¾Ğ±Ğ¾Ğ¸Ñ…:

```python
import os
import httpx
from typing import Optional

# Environment variables
OLLAMA_ENDPOINT = os.getenv("OLLAMA_ENDPOINT", "http://localhost:11434")
LM_STUDIO_ENDPOINT = os.getenv("LM_STUDIO_ENDPOINT", "http://localhost:1234")

class HybridModelRouter:
    """Router Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Ollama Ğ¸ LM Studio"""
    
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=120.0)
        self.preferred_llm = os.getenv("PREFERRED_LLM", "ollama")  # ollama, lm_studio, auto
    
    async def call_local_llm(
        self,
        prompt: str,
        model_name: Optional[str] = None,
        **kwargs
    ) -> dict:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ LLM (Ollama Ğ¸Ğ»Ğ¸ LM Studio)"""
        
        # ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ Ğ¸Ğ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹
        if self.preferred_llm == "auto":
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ
            ollama_available = await self._check_service(OLLAMA_ENDPOINT)
            lm_studio_available = await self._check_service(LM_STUDIO_ENDPOINT)
            
            if lm_studio_available:
                return await self._call_lm_studio(prompt, model_name, **kwargs)
            elif ollama_available:
                return await self._call_ollama(prompt, model_name, **kwargs)
            else:
                raise Exception("No local LLM service available")
        
        elif self.preferred_llm == "lm_studio":
            return await self._call_lm_studio(prompt, model_name, **kwargs)
        else:
            return await self._call_ollama(prompt, model_name, **kwargs)
    
    async def _check_service(self, endpoint: str) -> bool:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ°"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{endpoint}/v1/models", timeout=2.0)
                return response.status_code == 200
        except:
            return False
    
    async def _call_lm_studio(
        self,
        prompt: str,
        model: Optional[str],
        **kwargs
    ) -> dict:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² LM Studio API"""
        url = f"{LM_STUDIO_ENDPOINT}/v1/chat/completions"
        
        payload = {
            "model": model or "local-model",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 2000),
        }
        
        try:
            response = await self.client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()
            
            return {
                "response": data["choices"][0]["message"]["content"],
                "tokens": data.get("usage", {}).get("total_tokens", 0),
                "model": data.get("model", "lm-studio"),
            }
        except httpx.HTTPError as e:
            raise Exception(f"LM Studio error: {e}")
    
    async def _call_ollama(self, prompt: str, model: Optional[str], **kwargs) -> dict:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² Ollama API"""
        url = f"{OLLAMA_ENDPOINT}/api/generate"
        
        payload = {
            "model": model or "llama3.1:8b",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.7),
                "num_predict": kwargs.get("max_tokens", 2000),
            }
        }
        
        try:
            response = await self.client.post(url, json=payload)
            response.raise_for_status()
            data = response.json()
            
            return {
                "response": data.get("response", ""),
                "tokens": data.get("eval_count", 0),
                "model": data.get("model", "ollama"),
            }
        except httpx.HTTPError as e:
            raise Exception(f"Ollama error: {e}")
```

---

## ğŸ“Š Ğ¨Ğ°Ğ³ 4: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° n8n workflow

Ğ’ n8n Ğ´Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ HTTP Request node Ğ´Ğ»Ñ LM Studio:

```json
{
  "name": "LM Studio LLM",
  "type": "n8n-nodes-base.httpRequest",
  "parameters": {
    "url": "http://localhost:1234/v1/chat/completions",
    "method": "POST",
    "authentication": "none",
    "sendBody": true,
    "specifyBody": "json",
    "jsonBody": "={\n  \"model\": \"local-model\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"{{ $json.body.prompt }}\"}],\n  \"temperature\": 0.7,\n  \"max_tokens\": 2000\n}",
    "options": {}
  },
  "position": [250, 300]
}
```

---

## ğŸ¯ Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ: LM Studio vs Ollama

| Feature | LM Studio | Ollama |
|---------|-----------|--------|
| **UI** | âœ… GUI Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ | âŒ CLI only |
| **API** | âœ… OpenAI-compatible | âœ… Custom API |
| **Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹** | âœ… Ğ§ĞµÑ€ĞµĞ· UI | âš ï¸ ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ° ollama pull |
| **GPU Support** | âœ… CUDA | âœ… CUDA |
| **Docker** | âŒ ĞĞµÑ‚ (GUI) | âœ… Ğ•ÑÑ‚ÑŒ |
| **Production** | âš ï¸ Ğ”Ğ»Ñ dev | âœ… Ğ”Ğ° |

**Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ:**
- **Ğ›Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°**: LM Studio (ÑƒĞ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ UI)
- **Production/Ğ¡ĞµÑ€Ğ²ĞµÑ€**: Ollama (Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹, Docker-friendly)

---

## ğŸ”„ Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Production (ĞĞ½Ğ»Ğ°Ğ¹Ğ½)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ASUS TUF Laptop                       â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ LM Studio    â”‚  â”‚ Ollama       â”‚   â”‚
â”‚  â”‚ (Dev mode)   â”‚  â”‚ (Production) â”‚   â”‚
â”‚  â”‚ Port 1234    â”‚  â”‚ Port 11434   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                  â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                  â–¼                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚ FastAPI Router  â”‚            â”‚
â”‚         â”‚ Auto-detect     â”‚            â”‚
â”‚         â”‚ preferred LLM   â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                        â”‚
â”‚  Smart routing:                        â”‚
â”‚  â€¢ Code tasks â†’ LM Studio              â”‚
â”‚  â€¢ Chat tasks â†’ Ollama                 â”‚
â”‚  â€¢ Fallback Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½Ğ¸Ğ¼Ğ¸                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°

### 1. Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚Ğµ LM Studio API

Ğ’ LM Studio:
- Local Server â†’ Start Server
- ĞŸĞ¾Ñ€Ñ‚: 1234
- ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ: `curl http://localhost:1234/v1/models`

### 2. ĞĞ±Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ

```bash
# Ğ’ .env Ñ„Ğ°Ğ¹Ğ»Ğµ
PREFERRED_LLM=lm_studio
LM_STUDIO_ENDPOINT=http://localhost:1234
```

### 3. ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ API

```bash
docker restart ai-platform-api
```

### 4. Ğ¢ĞµÑÑ‚

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write Python hello world",
    "task_type": "code"
  }'
```

---

## ğŸ“ ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

### Python ĞºĞ»Ğ¸ĞµĞ½Ñ‚

```python
import httpx

async def test_lm_studio():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:1234/v1/chat/completions",
            json={
                "model": "local-model",
                "messages": [{"role": "user", "content": "Hello!"}]
            }
        )
        print(response.json())

# import asyncio; asyncio.run(test_lm_studio())
```

### REST API

```bash
# Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
curl http://localhost:1234/v1/models

# Chat completion
curl -X POST http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-model",
    "messages": [{"role": "user", "content": "Say hi"}]
  }'
```

---

## âœ… Checklist

- [ ] LM Studio Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½
- [ ] Local Server Ğ²ĞºĞ»ÑÑ‡ĞµĞ½ (Ğ¿Ğ¾Ñ€Ñ‚ 1234)
- [ ] ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ² LM Studio
- [ ] API Ñ‚ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½: `curl http://localhost:1234/v1/models`
- [ ] Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ğ² FastAPI (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾)
- [ ] n8n workflow Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½
- [ ] Ğ¢ĞµÑÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ ÑÑ‚ĞµĞºĞ°

---

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ LM Studio:**

1. **Ğ£Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ GUI** Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
2. **Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ğ¾ÑÑ‚ÑŒ** - Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒÑÑ Ğ¼ĞµĞ¶Ğ´Ñƒ LM Studio Ğ¸ Ollama
3. **Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ** - OpenAI API format
4. **ĞœĞ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ** - Ğ»ÑƒÑ‡ÑˆĞ°Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹

**Ğ˜Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ†ĞµĞ½Ğ°Ñ€Ğ¸Ğ¹:**
- **Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°**: LM Studio (GUI ÑƒĞ´Ğ¾Ğ±ĞµĞ½ Ğ´Ğ»Ñ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²)
- **Production**: Ollama (Docker, Ğ»ĞµĞ³ĞºĞ¾Ğ²ĞµÑĞ½Ñ‹Ğ¹)

